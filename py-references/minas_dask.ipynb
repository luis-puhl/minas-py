{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puhl/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import logging, scipy, dask, pandas, numpy, time, typing, asyncio\n",
    "import dask.dataframe as ddf\n",
    "import dask.bag as dbag\n",
    "import dask.distributed as dask_distributed\n",
    "from dask.distributed import Client\n",
    "import distributed.diagnostics as daskdia\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "numpy.random.seed(201)\n",
    "k = 100\n",
    "representationThr = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = dask_distributed.Client()\n",
    "\n",
    "# StreamZ Dask\n",
    "client = await Client(processes=False)\n",
    "\n",
    "# StreamZ Dask Async\n",
    "# client = await Client(processes=False, asynchronous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [{'label': f'Class #{i}', 'mu': numpy.random.random() * 10, 'sigma': numpy.random.random() * 5} for i in range(5)]\n",
    "def nextExample():\n",
    "    cl = classes[numpy.random.randint(0, len(classes))]\n",
    "    item = [float(numpy.random.normal(loc=cl['mu'], scale=cl['sigma'])) for i in range(40)]\n",
    "    return {'label': cl['label'], 'item': item,}\n",
    "\n",
    "def limit(stream, limit):\n",
    "    msg = yield\n",
    "    for target_list in range(limit):\n",
    "        yield next(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# examples = dbag.from_sequence(limit(mkStream(), 10000))\n",
    "# daskdia.progress(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3046a641abf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtornado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mioloop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIOLoop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstreamz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mStreamzDataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamz'"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from tornado.ioloop import IOLoop\n",
    "import streamz\n",
    "from streamz.dataframe import DataFrame as StreamzDataframe\n",
    "\n",
    "exStream = streamz.Stream()\n",
    "# exStream = streamz.Stream(asynchronous=True)\n",
    "def aggOffile(accDF, example):\n",
    "    if example is None:\n",
    "        return accDF\n",
    "    return accDF.append(pandas.DataFrame({'label': [example['label']], 'item': [example['item']]}))\n",
    "offlineAgg = exStream.accumulate(aggOffile, start=pandas.DataFrame({'label': [], 'item': []}), returns_state=False)\n",
    "\n",
    "# sdf = StreamzDataframe(source,)\n",
    "# # buffer(5)\n",
    "# dfStream = source.map(lambda ex: pandas.dataframe(data=ex))\n",
    "# dfStream.accumulate().map(lambda ex: pandas.dataframe(data=ex)).scatter().map(increment).gather().sink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(offlineAgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offlineAgg.sink(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exStream.emit(nextExample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "for i in range(10000):\n",
    "    exStream.emit(nextExample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate():\n",
    "    for i in range(10000):\n",
    "        await asyncio.sleep(10)\n",
    "        await exStream.emit(nextExample())\n",
    "    await exStream.emit(None)\n",
    "# asyncio.get_loop().run(generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster():\n",
    "    def __init__(self, center=[]):\n",
    "        self.center = center\n",
    "        self.label = None\n",
    "        self.n = 0\n",
    "        self.lastExapleTMS = 0\n",
    "        self.sumDistance = 0\n",
    "        self.maxDistance = 0\n",
    "    def dist(self, item):\n",
    "        return scipy.spatial.distance.euclidean(self.center, item)\n",
    "    def addExample(self, item, distance=None):\n",
    "        self.n += 1\n",
    "        if example.timestamp > self.lastExapleTMS:\n",
    "              self.lastExapleTMS = example.timestamp\n",
    "        if distance == None:\n",
    "              distance = self.dist(item)\n",
    "        self.sumDistance += distance\n",
    "        if distance > self.maxDistance:\n",
    "              self.maxDistance = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(examples, label=None):\n",
    "    representationThr = 3\n",
    "    kmeans = KMeans( n_clusters = min(k, int(len(examples) / (3 * representationThr))) )\n",
    "    with joblib.parallel_backend('dask'):\n",
    "        kmeans.fit(examples)\n",
    "    return [Cluster(center=centroid, label=label) for centroid in kmeans.cluster_centers_]\n",
    "def closestCluster(item, clusters):\n",
    "    return min([ (cl, cl.dist(item)) for cl in clusters ], key=lambda x: x[1])\n",
    "def offline(label, group):\n",
    "    representationThr = 3\n",
    "    clusters = clustering(group, label)\n",
    "    for ex in group:\n",
    "        nearCl, dist = closestCluster(ex, clusters)\n",
    "        nearCl.addExample(ex)\n",
    "    return [cluster for cluster in clusters if cluster.n > representationThr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(i, downStream=streamz.Stream()):\n",
    "    _, df = i\n",
    "    return [ downStream.emit( (l, df[g['item']]) ) for l, g in df.groupby('label') ]\n",
    "def mapOffline(i):\n",
    "    label, group = i\n",
    "    return offline(label, group)\n",
    "\n",
    "trainingTrigger = streamz.Stream()\n",
    "groups = streamz.Stream()\n",
    "\n",
    "L = []\n",
    "def nnnn(i):\n",
    "    L.append(i)\n",
    "trainingTrigger.zip(offlineAgg).sink(nnnn)\n",
    "# grouped = trainingTrigger.zip(offlineAgg).map(group, downStream=groups)\n",
    "# grouped.sink(lambda x: print('grouped {x!r} {t}'.format(x=x, t=type(x))))\n",
    "\n",
    "# trained = groups.map(mapOffline)\n",
    "# trained = groups.scatter().map(mapOffline).gather()\n",
    "# trained.sink(lambda x: print('trained {x!r} {t}'.format(x=x, t=type(x))))\n",
    "\n",
    "trainingTrigger.emit(True)\n",
    "exStream.emit(nextExample())\n",
    "trained.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingTrigger.emit(True)\n",
    "# trained.sink(print)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamz.dataframe import Random, DataFrame\n",
    "%matplotlib inline\n",
    "\n",
    "source = Random(freq='5ms', interval='100ms')\n",
    "r = source.x.sum()\n",
    "r.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = (source - 0.5).cumsum()\n",
    "sdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (DataFrame({'raw': sdf.x,\n",
    "                        'smooth': sdf.x.rolling('100ms').mean(),\n",
    "                        'very-smooth': sdf.x.rolling('500ms').mean()})\n",
    "     .plot(width=700)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamz\n",
    "def increment(x):\n",
    "    return x + 1\n",
    "\n",
    "def decrement(x):\n",
    "    return x - 1\n",
    "\n",
    "s = streamz.Stream()\n",
    "a = s.map(increment).sink(print)\n",
    "b = s.map(decrement).sink(print)\n",
    "b.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
