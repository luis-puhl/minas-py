{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import typing\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Vector = typing.Union[list, np.ndarray, typing.Any, None]\n",
    "\n",
    "@dataclasses.dataclass(eq=False)\n",
    "class Example():\n",
    "    item: Vector = dataclasses.field(repr=False)\n",
    "    label: typing.Union[str, None] = None\n",
    "    timestamp: int = time.time_ns()\n",
    "    tries: int = 0\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Cluster():\n",
    "    center: Vector = dataclasses.field(repr=False)\n",
    "    latest: int = 0\n",
    "    label: typing.Union[str, None] = None\n",
    "    n: int = 0\n",
    "    maxDist: float = 0.0\n",
    "    temp_examples=[]\n",
    "    timestamp: int = time.time_ns()\n",
    "    def __eq__(self, other):\n",
    "        return self.timestamp == other.timestamp\n",
    "    def __hash__(self):\n",
    "        return self.timestamp\n",
    "\n",
    "def nextExample(klass): return Example(label=klass['label'], item=np.random.normal(klass['mu'], klass['sigma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNKNOWN] 0: [55.01325777 43.43804642]',\n",
       " '[UNKNOWN] 1: [25.54970639 79.46046958]',\n",
       " '[UNKNOWN] 2: [65.98866569 31.19866669]',\n",
       " '[UNKNOWN] 3: [129.34893922  79.81543607]',\n",
       " '[UNKNOWN] 4: [19.710006   65.09775844]',\n",
       " '[UNKNOWN] 5: [20.75774943 80.09968697]',\n",
       " '[UNKNOWN] 6: [96.60175838 49.0629403 ]',\n",
       " '[UNKNOWN] 7: [111.67200986  61.6094315 ]',\n",
       " '[UNKNOWN] 8: [19.28188458 94.9561484 ]',\n",
       " '[UNKNOWN] 9: [ 4.1458068  54.01927865]']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mkClass(label): return dict(label=label, mu=np.random.random_sample((2,)) * 100, sigma=np.random.random_sample((2,)) * 100)\n",
    "classes = list(map(mkClass, ['zero', 'one', 'duo', 'tri']))\n",
    "list(map(nextExample, classes))\n",
    "\n",
    "def nextRandExample(classes=classes): return nextExample(np.random.choice(classes) )\n",
    "def randExamplesIter(classes=classes):\n",
    "    while True:\n",
    "        yield nextRandExample(classes=classes)\n",
    "# \n",
    "def loopExamplesIter(classes=classes):\n",
    "    i = 0\n",
    "    while True:\n",
    "        msg = yield nextExample(classes[i])\n",
    "        if not msg is None:\n",
    "            classes = msg\n",
    "        i = (i + 1) % len(classes)\n",
    "# \n",
    "def distKlass(ex, classes=classes): return map(lambda cl: (sum((cl['mu'] - ex.item) ** 2) ** 1/2, cl), classes)\n",
    "def minDistKlass(ex, classes=classes): return min(distKlass(ex, classes), key=lambda x: x[0])\n",
    "minDistKlass(nextExample(classes[0]))\n",
    "\n",
    "clusters = [ Cluster(center=cl['mu'], label=cl['label'], n=0, maxDist=sum(cl['sigma']), latest=0) for cl in classes ]\n",
    "unkBuff = []\n",
    "outStream = []\n",
    "inStream = zip(range(10), loopExamplesIter())\n",
    "\n",
    "def dist(ex, clusters=clusters): return map(lambda cl: (sum((cl.center - ex.item) ** 2) ** 1/2, cl), clusters)\n",
    "def minDist(ex, clusters=clusters): return min(dist(ex, clusters), key=lambda x: x[0])\n",
    "for i, example in inStream:\n",
    "    d, cl = minDist(example)\n",
    "    if d / cl.maxDist <= 1.1:\n",
    "        outStream.append(f'[CLASSIFIED] {i}: {cl.label}')\n",
    "        cl.maxDist = max(cl.maxDist, d)\n",
    "    else:\n",
    "        outStream.append(f'[UNKNOWN] {i}: {example.item}')\n",
    "        unkBuff.append(example)\n",
    "outStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def minDist(clusters, item):\n",
    "    dists = map(lambda cl: (sum((cl.center - item) ** 2) ** (1/2), cl), clusters)\n",
    "    d, cl = min(dists, key=lambda x: x[0])\n",
    "    return d, cl\n",
    "def clustering(unknownBuffer, label=None, MAX_K_CLUSTERS=100, REPR_TRESHOLD=20):\n",
    "    df = pd.DataFrame([ex.item for ex in unknownBuffer])\n",
    "    n_clusters = min(MAX_K_CLUSTERS, len(unknownBuffer) // ( 3 * REPR_TRESHOLD))\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(df)\n",
    "    newClusters = [Cluster(center=centroid, label=label, n=0, maxDist=0, latest=0) for centroid in kmeans.cluster_centers_]\n",
    "    return newClusters\n",
    "\n",
    "def minasOnline(exampleSource, inClusters=[], minDist=minDist, clustering=clustering):\n",
    "    RADIUS_FACTOR = 1.1\n",
    "    BUFF_FULL = 100\n",
    "    MAX_K_CLUSTERS = 100\n",
    "    REPR_TRESHOLD = 20\n",
    "    CLEANUP_WINDOW = 100\n",
    "    #\n",
    "    unknownBuffer = []\n",
    "    clusters=[cl for cl in inClusters]\n",
    "    sleepClusters = []\n",
    "    counter = 0\n",
    "    noveltyIndex = 0\n",
    "    sentinel = object()\n",
    "    while True:\n",
    "        example = next(exampleSource, sentinel)\n",
    "        if example is sentinel:\n",
    "            yield 'done'\n",
    "            return\n",
    "        example = Example(item=example.item)\n",
    "        counter += 1\n",
    "        example.timestamp = time.time_ns()\n",
    "        example.n = counter\n",
    "        # dists = map(lambda cl: (sum((cl.center - example.item) ** 2) ** 1/2, cl), clusters)\n",
    "        # d, cl = min(dists, key=lambda x: x[0])\n",
    "        d, cl = minDist(clusters, example.item)\n",
    "        if d / cl.maxDist <= RADIUS_FACTOR:\n",
    "            cl.maxDist = max(cl.maxDist, d)\n",
    "            cl.latest = counter\n",
    "            cl.n += 1\n",
    "            yield f\"[CLASSIFIED] {example.n}: {cl.label}\"\n",
    "        else:\n",
    "            unknownBuffer.append(example)\n",
    "            yield f\"[UNKNOWN] {example.n}: {example.item}\"\n",
    "            if len(unknownBuffer) > BUFF_FULL:\n",
    "                if len(sleepClusters) > 0:\n",
    "                    yield f'[recurenceDetection] unk={len(unknownBuffer)}, sleep={len(sleepClusters)}'\n",
    "                    # recurenceDetection\n",
    "                    for sleepExample in unknownBuffer:\n",
    "                        # sleepDists = list(map(lambda cl: (sum((cl.center - sleepExample.item) ** 2) ** 1/2, cl), sleepClusters))\n",
    "                        # d, cl = min(sleepDists, key=lambda x: x[0])\n",
    "                        d, cl = minDist(sleepClusters, sleepExample.item)\n",
    "                        if d / cl.maxDist <= 1.1:\n",
    "                            cl.maxDist = max(cl.maxDist, d)\n",
    "                            cl.latest = counter\n",
    "                            unknownBuffer.remove(sleepExample)\n",
    "                            yield f\"[CLASSIFIED] {sleepExample.n}: {cl.label}\"\n",
    "                            if cl in sleepClusters:\n",
    "                                clusters.append(cl)\n",
    "                                sleepClusters.remove(cl)\n",
    "                                yield f\"[Recurence] {cl.label}\"\n",
    "                if len(unknownBuffer) % (BUFF_FULL // 10) == 0:\n",
    "                    yield '[noveltyDetection]'\n",
    "                    # noveltyDetection\n",
    "                    # df = pd.DataFrame([ex.item for ex in unknownBuffer])\n",
    "                    # n_clusters = min(MAX_K_CLUSTERS, len(unknownBuffer) // ( 3 * REPR_TRESHOLD))\n",
    "                    # kmeans = KMeans(n_clusters=n_clusters)\n",
    "                    # kmeans.fit(df)\n",
    "                    # newClusters = [Cluster(center=centroid, label=None, n=0, maxDist=0, latest=0) for centroid in kmeans.cluster_centers_]\n",
    "                    newClusters = clustering(unknownBuffer)\n",
    "                    temp_examples = {cl: [] for cl in newClusters}\n",
    "                    for sleepExample in unknownBuffer:\n",
    "                        # dists = map(lambda cl: (sum((cl.center - sleepExample.item) ** 2) ** 1/2, cl), newClusters)\n",
    "                        # d, cl = min(dists, key=lambda x: x[0])\n",
    "                        d, cl = minDist(newClusters, sleepExample.item)\n",
    "                        cl.maxDist = max(cl.maxDist, d)\n",
    "                        cl.latest = counter\n",
    "                        cl.n += 1\n",
    "                        temp_examples[cl].append((sleepExample, d))\n",
    "                    for ncl in newClusters:\n",
    "                        if ncl.n < 2: continue\n",
    "                        distances = [ d for ex, d in temp_examples[ncl] ]\n",
    "                        if len(distances) == 0: continue\n",
    "                        # distsCl2Cl = map(lambda cl: (sum((cl.center - ncl.center) ** 2) ** 1/2, cl), clusters + sleepClusters)\n",
    "                        # distCl2Cl, nearCl2Cl = min(distsCl2Cl, key=lambda x: x[0])\n",
    "                        distCl2Cl, nearCl2Cl = minDist(clusters + sleepClusters, ncl.center)\n",
    "                        #\n",
    "                        mean = sum(distances) / len(distances)\n",
    "                        devianceSqrSum = sum([(d - mean) **2 for d in distances])\n",
    "                        var = devianceSqrSum / len(distances)\n",
    "                        stdDevDistance = var **0.5\n",
    "                        silhouetteFn = lambda a, b: (b - a) / max([a, b])\n",
    "                        silhouette = silhouetteFn(stdDevDistance, distCl2Cl)\n",
    "                        if silhouette < 0: continue\n",
    "                        # \n",
    "                        sameLabel = [ cl for cl in clusters + sleepClusters if cl.label ==  nearCl2Cl.label ]\n",
    "                        sameLabelDists = [ sum((cl1.center - cl2.center) ** 2) ** (1/2) for cl1, cl2 in itertools.combinations(sameLabel, 2) ]\n",
    "                        #\n",
    "                        if distCl2Cl / nearCl2Cl.maxDist < 1.1 or distCl2Cl / max(sameLabelDists) < 2:\n",
    "                            yield f'Extention {nearCl2Cl.label}'\n",
    "                            ncl.label = nearCl2Cl.label\n",
    "                        else:\n",
    "                            label = 'Novelty {}'.format(noveltyIndex)\n",
    "                            ncl.label = label\n",
    "                            yield label\n",
    "                            noveltyIndex += 1\n",
    "                        clusters.append(ncl)\n",
    "                        for ex, d in temp_examples[ncl]:\n",
    "                            if ex in unknownBuffer:\n",
    "                                yield f\"[CLASSIFIED] {ex.n}: {ncl.label}\"\n",
    "                                unknownBuffer.remove(ex)\n",
    "        if counter % CLEANUP_WINDOW == 0:\n",
    "            yield '[cleanup]'\n",
    "            for ex in unknownBuffer:\n",
    "                if counter - ex.n < 3 * CLEANUP_WINDOW:\n",
    "                    unknownBuffer.remove(ex)\n",
    "            for cl in clusters:\n",
    "                if counter - cl.latest < 2 * CLEANUP_WINDOW:\n",
    "                    sleepClusters.append(cl)\n",
    "                    clusters.remove(cl)\n",
    "            if len(clusters) == 0:\n",
    "                yield f'[fallback] {len(sleepClusters)} => clusters'\n",
    "                # fallback \n",
    "                clusters.extend(sleepClusters)\n",
    "                sleepClusters.clear()\n",
    "            #\n",
    "        #\n",
    "    #\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaMinas(minasMaping):\n",
    "    status = dict(\n",
    "        known = 0,\n",
    "        unknown = 0,\n",
    "        cleanup = 0,\n",
    "        fallback = 0,\n",
    "        recurenceDetection = 0,\n",
    "        recurence = 0,\n",
    "        noveltyDetection = 0,\n",
    "    )\n",
    "    sentinel = object()\n",
    "    while minasMaping:\n",
    "        o = next(minasMaping, sentinel)\n",
    "        if o == sentinel:\n",
    "            break\n",
    "        if '[CLASSIFIED]' in o:\n",
    "            status['known'] += 1\n",
    "        elif '[UNKNOWN]' in o:\n",
    "            status['unknown'] += 1\n",
    "        elif '[cleanup]' in o:\n",
    "            status['cleanup'] += 1\n",
    "        elif '[fallback]' in o:\n",
    "            status['fallback'] += 1\n",
    "        elif '[recurenceDetection]' in o:\n",
    "            status['recurenceDetection'] += 1\n",
    "        elif '[noveltyDetection]' in o:\n",
    "            status['noveltyDetection'] += 1\n",
    "        elif '[Recurence]' in o:\n",
    "            status['recurence'] += 1\n",
    "        else: \n",
    "            yield o\n",
    "    else:\n",
    "        yield 'Stream Done'\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(map(mkClass, ['zero', 'one', 'duo', 'tri']))\n",
    "clusters = [ Cluster(center=cl['mu'], label=cl['label'], n=0, maxDist=sum(cl['sigma']), latest=0) for cl in classes ]\n",
    "    \n",
    "inputStream = loopExamplesIter()\n",
    "for kl in range(10):\n",
    "    for i, o in zip(range(10), metaMinas(minasOnline(inputStream, clusters))):\n",
    "        print(o)\n",
    "    newClass = mkClass(f'New {kl}')\n",
    "    print(newClass)\n",
    "    classes.append(newClass)\n",
    "    inputStream.send(classes)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minasOffline(examplesDf):\n",
    "    RADIUS_FACTOR = 1.1\n",
    "    BUFF_FULL = 100\n",
    "    MAX_K_CLUSTERS = 100\n",
    "    REPR_TRESHOLD = 20\n",
    "    #\n",
    "    clusters = []\n",
    "    groupSize = MAX_K_CLUSTERS * REPR_TRESHOLD\n",
    "    for label, group in examplesDf.groupby('label'):\n",
    "        for chunk in range(0, len(group), groupSize):\n",
    "            subgroup = group[chunk:chunk + groupSize]\n",
    "            unknownBuffer = list(subgroup['item'])\n",
    "            df = pd.DataFrame(unknownBuffer)\n",
    "            n_clusters = min(MAX_K_CLUSTERS, len(unknownBuffer) // ( 3 * REPR_TRESHOLD))\n",
    "            kmeans = KMeans(n_clusters=n_clusters)\n",
    "            kmeans.fit(df)\n",
    "            newClusters = [Cluster(center=centroid, label=label, n=0, maxDist=0, latest=0) for centroid in kmeans.cluster_centers_]\n",
    "            temp_examples = {cl: [] for cl in newClusters}\n",
    "            for sleepExample in unknownBuffer:\n",
    "                dists = map(lambda cl: (sum((cl.center - sleepExample) ** 2) ** 1/2, cl), newClusters)\n",
    "                d, cl = min(dists, key=lambda x: x[0])\n",
    "                cl.maxDist = max(cl.maxDist, d)\n",
    "                cl.n += 1\n",
    "                temp_examples[cl].append((sleepExample, d))\n",
    "            for ncl in newClusters:\n",
    "                if ncl.n < 2: continue\n",
    "                #\n",
    "                clusters.append(ncl)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "from sklearn.datasets import fetch_covtype\n",
    "covtype = fetch_covtype()\n",
    "total = len(covtype.data)\n",
    "\n",
    "zipToMap = lambda x: {'item': x[0], 'label': str(x[1])}\n",
    "onePercent = int(total*0.01)\n",
    "baseMap = map(zipToMap, zip(covtype.data[:onePercent], covtype.target[:onePercent]))\n",
    "onPercentDataFrame = pd.DataFrame(baseMap)\n",
    "\n",
    "clusters = minasOffline(onPercentDataFrame)\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fivePercent = int(total*0.05)\n",
    "fivePercentZip = zip(dataset.data[onePercent+1:fivePercent], map(str, dataset.target[onePercent+1:fivePercent]))\n",
    "inputStream = ( Example(item=i, label=t) for i, t in fivePercentZip)\n",
    "for o in metaMinas(minasOnline(inputStream, clusters)):\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 b'tcp' b'http' b'SF' 181 5450 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 8 8 0.0\n",
      " 0.0 0.0 0.0 1.0 0.0 0.0 9 9 1.0 0.0 0.11 0.0 0.0 0.0 0.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "kddcup99 = fetch_kddcup99()\n",
    "total = len(kddcup99.data)\n",
    "online = 442800\n",
    "offline = 48791\n",
    "total - online - offline\n",
    "\n",
    "print(kddcup99.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
